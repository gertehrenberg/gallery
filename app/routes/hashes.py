import asyncio
import hashlib
import json
import os
from pathlib import Path
from typing import Dict, List, Any
from typing import Optional

from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload

from app.config import Settings
from app.config_gdrive import sanitize_filename, folder_id_by_name, SettingsGdrive, collect_all_folders
from app.routes.auth import load_drive_service_token
from app.routes.gdrive_from_lokal import save_structured_hashes
from app.tools import readimages
from app.utils.db_utils import save_folder_status_to_db, load_folder_status_from_db
from app.utils.progress import init_progress_state, progress_state, update_progress, update_progress_text, \
    save_simple_hashes, hold_progress, stop_progress, write_local_hashes_progress


def is_valid_image(filename: str) -> bool:
    """PrÃ¼ft, ob eine Datei eine gÃ¼ltige Bilddatei ist."""
    return any(filename.lower().endswith(ext) for ext in Settings.IMAGE_EXTENSIONS)


async def delete_all_hashfiles_async(file_folder_dir: str, subfolders: bool = True) -> int:
    """LÃ¶scht alle Hash-Dateien in einem Verzeichnis und optional seinen Unterverzeichnissen."""
    await init_progress_state()
    await update_progress_text("ðŸ”„ Starte LÃ¶schvorgang der Hash-Dateien...")

    root = Path(file_folder_dir)
    all_dirs = [root] if not subfolders else [root] + [d for d in root.iterdir() if d.is_dir()]
    deleted = 0
    total_dirs = len(all_dirs)

    for idx, subdir in enumerate(all_dirs, 1):
        await update_progress(f"Verarbeite Verzeichnis {subdir.name}", int((idx / total_dirs) * 100))

        for file in subdir.glob(f"*{Settings.GDRIVE_HASH_FILE}"):
            try:
                file.unlink()
                await update_progress_text(f"ðŸ—‘ï¸ GelÃ¶scht: {file}")
                deleted += 1
            except Exception as e:
                await update_progress_text(f"âŒ Fehler beim LÃ¶schen von {file}: {e}")

    await update_progress_text(f"âœ… Hash-Dateien gelÃ¶scht: {deleted}")
    await hold_progress()
    return deleted


async def update_local_hashes(folder_name):
    local_cache = {}
    folder_path = os.path.join(Settings.IMAGE_FILE_CACHE_DIR, folder_name)

    await update_progress_text(f"ðŸ“ Verarbeite Kategorie: {folder_name}")

    await readimages(folder_path, local_cache)

    for image_name, entry in local_cache.items():
        image_id = entry.get('image_id')
        if image_id:
            save_folder_status_to_db(Settings.DB_PATH, image_id, folder_name)

    hash_file = Path(folder_path) / Settings.GALLERY_HASH_FILE
    local_hashes = {name: data.get('image_id', '') for name, data in local_cache.items()}

    try:
        await save_simple_hashes(local_hashes, hash_file)
        await update_progress_text(f"âœ… {len(local_hashes)} Hashes gespeichert fÃ¼r {folder_name}")
    except Exception as e:
        await update_progress_text(f"âŒ Fehler beim Speichern der Hashes fÃ¼r {folder_name}: {e}")


async def update_all_local_hashes():
    """Aktualisiert die Hash-Dateien fÃ¼r alle Kategorien."""
    await init_progress_state()
    progress_state["running"] = True

    try:
        total_kategorien = len(Settings.kategorien)

        for idx, kategorie in enumerate(Settings.kategorien, 1):
            folder_name = kategorie["key"]
            folder_path = os.path.join(Settings.IMAGE_FILE_CACHE_DIR, folder_name)

            await update_progress(f"Kategorie {folder_name}", int((idx / total_kategorien) * 100))
            await update_local_hashes(folder_path)

        await update_progress_text("âœ… Hash-Aktualisierung abgeschlossen")

    except Exception as e:
        await update_progress_text(f"âŒ Fehler bei der Hash-Aktualisierung: {e}")
    finally:
        await hold_progress()


async def update_all_gdrive_hashes(service) -> None:
    """Aktualisiert die Hashes fÃ¼r alle Google Drive Ordner."""
    await update_progress_text("ðŸ”„ Starte Google Drive Hash-Aktualisierung...")

    total_kategorien = len(Settings.kategorien)
    for idx, kategorie in enumerate(Settings.kategorien, 1):
        folder_name = kategorie["key"]
        folder_id = folder_id_by_name(folder_name)

        if not folder_id:
            await update_progress_text(f"âš ï¸ Keine Folder-ID gefunden fÃ¼r: {folder_name}")
            continue

        await update_progress(f"Verarbeite {folder_name}", int((idx / total_kategorien) * 100))
        await update_gdrive_hashes(service, folder_name)


async def update_gdrive_hashes(service, folder_name: Optional[str] = None):
    """Liest Dateien aus Google Drive und aktualisiert die hashes.json Dateien."""
    await update_progress_text(f"ðŸ”„ Aktualisiere GDrive Hashes{' fÃ¼r ' + folder_name if folder_name else ''}")
    await init_progress_state()
    progress_state["running"] = True

    try:
        base_dir = Path(Settings.IMAGE_FILE_CACHE_DIR)
        folders_to_process = [base_dir / folder_name] if folder_name else [p for p in base_dir.iterdir() if p.is_dir()]
        total_folders = len(folders_to_process)

        for folder_idx, folder_path in enumerate(sorted(folders_to_process), 1):
            current_folder = folder_path.name
            folder_id = folder_id_by_name(current_folder)

            if not folder_id:
                await update_progress_text(f"âš ï¸ Keine Folder-ID gefunden fÃ¼r: {current_folder}")
                continue

            await update_progress_text(f"ðŸ“‚ Verarbeite {current_folder} ({folder_idx}/{total_folders})")

            gdrive_hashes: Dict[str, Dict[str, str]] = {}
            page_token = None
            files_processed = 0

            while True:
                try:
                    results = service.files().list(
                        q=f"'{folder_id}' in parents and trashed=false",
                        spaces='drive',
                        fields="nextPageToken, files(id, name, md5Checksum)",
                        pageSize=Settings.PAGESIZE,
                        pageToken=page_token
                    ).execute()

                    for item in results.get('files', []):
                        name = sanitize_filename(item.get('name', ''))
                        if name and is_valid_image(name):
                            md5_checksum = item.get('md5Checksum')
                            file_id = item.get('id')
                            if md5_checksum and file_id:
                                gdrive_hashes[name] = {
                                    "md5": md5_checksum,
                                    "id": file_id
                                }
                                files_processed += 1

                        if files_processed % 100 == 0:
                            await update_progress(
                                f"âš¡ Dateien in {current_folder}: {files_processed}",
                                int((files_processed / (files_processed + 1)) * 100)
                            )

                    page_token = results.get('nextPageToken')
                    if not page_token:
                        break

                except Exception as e:
                    await update_progress_text(f"âŒ Fehler beim Lesen von {current_folder}: {e}")
                    break

            if not gdrive_hashes:
                gdrive_hashes = {}
            try:
                hash_file_path = folder_path / Settings.GDRIVE_HASH_FILE
                await save_structured_hashes(gdrive_hashes, hash_file_path)
                await update_progress_text(f"âœ… {current_folder}: {len(gdrive_hashes)} EintrÃ¤ge gespeichert")
            except Exception as e:
                await update_progress_text(f"âŒ Fehler beim Speichern fÃ¼r {current_folder}: {e}")

        await update_progress_text("âœ… GDrive Hash-Aktualisierung abgeschlossen")

    except Exception as e:
        await update_progress_text(f"âŒ Fehler bei Hash-Aktualisierung: {e}")
    finally:
        await hold_progress()


async def reloadcache_progress(service, folder_key: Optional[str] = None):
    """
    Reloads the cache for folders based on the folder_key parameter.

    Args:
        folder_key: Optional folder key.
                   If None, processes all categories.
                   If in CHECKBOX_CATEGORIES, processes only that folder.
                   If "textfiles", processes text files.
    """
    try:
        await init_progress_state()
        progress_state["running"] = True
        await update_progress_text(f"ðŸ”„ Starte reloadcache_progress fÃ¼r Ordner: {folder_key}")
        Settings.folders_loaded = 0

        if folder_key == "textfiles":
            await update_progress_text("ðŸ—ƒï¸ Modus: Textverarbeitung")
            await process_text_files()

        elif folder_key in Settings.CHECKBOX_CATEGORIES:
            await update_progress_text(f"ðŸ“‚ Modus: Einzelne Kategorie ({folder_key})")
            folder_name = folder_key
            await update_gdrive_hashes(service, folder_name)
            await update_local_hashes(folder_name)
            Settings.folders_loaded += 1

        else:
            await update_progress_text("ðŸ“‚ Modus: Alle Kategorien")
            pair_cache = Settings.CACHE.get("pair_cache")
            pair_cache.clear()

            for kategorie in Settings.kategorien:
                folder_name = kategorie["key"]
                await update_gdrive_hashes(service, folder_name)
                await update_local_hashes(folder_name)
                Settings.folders_loaded += 1

            await update_progress_text("ðŸ—ƒï¸ Modus: Textverarbeitung")
            await process_text_files()

            await process_image_folders_gdrive_progress(service, folder_key)
            await update_progress_text(f"[âœ“] Hash-Datei aktualisiert fÃ¼r {folder_key}")

    except Exception as e:
        await update_progress_text(f"âŒ Fehler beim Reload-Cache: {e}")
        raise
    finally:
        await update_progress_text("âœ… reloadcache_progress abgeschlossen")
        await stop_progress()


async def process_text_files():
    """Processes text files in the text directory."""
    await update_progress_text("ðŸ§® Schreibe lokale Hashes (Texte)")
    await write_local_hashes_progress(
        Settings.TEXT_EXTENSIONS,
        Settings.TEXT_FILE_CACHE_DIR,
        False
    )


async def process_image_folders_gdrive_progress(service, folder_name: str):
    folder_id = folder_id_by_name(folder_name)
    if not folder_id:
        raise ValueError(f"Keine Folder-ID gefunden fÃ¼r: {folder_name}")

    await update_progress_text(f"Lade Dateien aus {folder_name}...")
    files = []
    page_token = None

    while True:
        response = service.files().list(
            q=f"'{folder_id}' in parents and trashed = false",
            spaces='drive',
            fields="nextPageToken, files(id, name, md5Checksum)",
            pageSize=Settings.PAGESIZE,
            pageToken=page_token
        ).execute()

        batch = [f for f in response.get('files', [])
                 if is_valid_image(f.get('name', ''))]
        files.extend(batch)
        await update_progress_text(f"Geladen: {len(files)} Dateien...")

        page_token = response.get('nextPageToken')
        if not page_token:
            break

    gdrive_hashes = {}
    for file in files:
        try:
            name = sanitize_filename(file['name'])
            if is_valid_image(name):
                md5_drive = file.get("md5Checksum")
                if md5_drive:
                    gdrive_hashes[name] = {
                        "md5": md5_drive,
                        "id": file['id']
                    }
        except Exception as e:
            await update_progress_text(f"[Fehler] {file['name']}: {e}")

    hashfile_path = Path(Settings.IMAGE_FILE_CACHE_DIR) / folder_name / Settings.GDRIVE_HASH_FILE
    hashfile_path.parent.mkdir(parents=True, exist_ok=True)
    with hashfile_path.open("w", encoding="utf-8") as f:
        json.dump(gdrive_hashes, f, indent=2)
    await update_progress_text(f"Hash-Datei gespeichert fÃ¼r {folder_name}")


async def _process_image_files_progress(image_files, folder_key, file_parents_cache, db_path):
    folder_name = label = next((k["label"] for k in Settings.kategorien if k["key"] == folder_key), None)
    total = len(image_files)
    for index, image_file in enumerate(image_files):
        await update_progress(f"Kategorie: {folder_name} : {total} Dateien ({image_file})",
                              int(index / total * 100), 0.02)
        if not image_file.is_file() or image_file.suffix.lower() not in Settings.IMAGE_EXTENSIONS:
            continue
        image_name = image_file.name.lower()
        pair = Settings.CACHE["pair_cache"].get(image_name)
        if not pair:
            await update_progress_text(
                f"[_process_image_files_progress] âš ï¸ Kein Eintrag im pair_cache fÃ¼r: {image_name}")
            continue
        await update_progress_text(
            f"[_process_image_files_progress] âœ…ï¸ Eintrag im pair_cache fÃ¼r: {folder_key} / {image_name}")
        image_id = pair["image_id"]
        file_parents_cache[folder_key].append(image_id)

        save_folder_status_to_db(db_path, image_id, folder_key)


async def _load_file_parents_cache_from_db(db_path: str, file_parents_cache: dict) -> bool:
    rows = load_folder_status_from_db(db_path)
    if not rows:
        return False
    await update_progress_text("[fill_folder_cache] ðŸ“¦ Lade file_parents_cache aus der Datenbank...")
    for image_id, folder_key in rows:
        if folder_key not in file_parents_cache:
            Settings.folders_loaded += 1
            file_parents_cache[folder_key] = []
            await update_progress_text(
                f"[fill_folder_cache] âœ… Cache aus DB geladen: {Settings.folders_loaded}/{Settings.folders_total} {folder_key}")
        file_parents_cache[folder_key].append(image_id)
    if Settings.folders_loaded != Settings.folders_total:
        Settings.folders_loaded = Settings.folders_total
        await update_progress_text(
            f"[fill_folder_cache] âœ… Cache aus DB geladen: {Settings.folders_loaded}/{Settings.folders_total}")
    return True


async def verify_file_cache_consistency(base_dir, folder_name: Optional[str] = None) -> dict:
    """
    ÃœberprÃ¼ft die Konsistenz zwischen dem lokalen Cache und den tatsÃ¤chlichen Dateien im Ordner.

    Args:
        folder_name: Optional - Spezifischer Ordner zum ÃœberprÃ¼fen. Wenn None, werden alle Ordner Ã¼berprÃ¼ft.

    Returns:
        Dict mit Statistiken und Inkonsistenzen
    """
    await init_progress_state()
    await update_progress_text("ðŸ” Starte lokale Cache-KonsistenzprÃ¼fung...")

    result = {
        "statistics": {
            "total_files": 0,
            "cached_files": 0,
            "missing_in_cache": [],
            "missing_in_folder": [],
            "processed_folders": 0
        },
        "inconsistencies": {}
    }

    try:
        folders_to_check = ([base_dir / folder_name] if folder_name
                            else [p for p in base_dir.iterdir() if p.is_dir()])

        total_folders = len(folders_to_check)

        for folder_idx, folder_path in enumerate(folders_to_check, 1):
            current_folder = folder_path.name
            await update_progress_text(f"ðŸ“ PrÃ¼fe Ordner: {folder_path} ({folder_idx}/{total_folders})")

            # Sammle alle Dateien im Ordner
            folder_files = set()
            try:
                folder_files = {
                    f.name.lower() for f in folder_path.iterdir()
                    if f.is_file() and f.suffix.lower() in Settings.IMAGE_EXTENSIONS
                }
            except Exception as e:
                await update_progress_text(f"âš ï¸ Fehler beim Lesen von {current_folder}: {e}")
                continue

            # Sammle alle Dateien im lokalen Hash-Cache
            hash_cache_path = folder_path / Settings.GALLERY_HASH_FILE
            cached_files = set()

            try:
                if hash_cache_path.exists():
                    with hash_cache_path.open('r') as f:
                        cache_data = json.load(f)
                        cached_files = {name.lower() for name in cache_data.keys()}
            except Exception as e:
                await update_progress_text(f"âš ï¸ Fehler beim Lesen der Cache-Datei fÃ¼r {current_folder}: {e}")
                continue

            # Finde Unterschiede
            missing_in_cache = folder_files - cached_files
            missing_in_folder = cached_files - folder_files

            # Aktualisiere Statistiken
            result["statistics"]["total_files"] += len(folder_files)
            result["statistics"]["cached_files"] += len(cached_files)
            result["statistics"]["processed_folders"] += 1

            if missing_in_cache or missing_in_folder:
                result["inconsistencies"][current_folder] = {
                    "missing_in_cache": list(missing_in_cache),
                    "missing_in_folder": list(missing_in_folder)
                }

                if missing_in_cache:
                    await update_progress_text(
                        f"âš ï¸ {len(missing_in_cache)} Dateien in {current_folder} nicht im Cache"
                    )
                if missing_in_folder:
                    await update_progress_text(
                        f"âš ï¸ {len(missing_in_folder)} Cache-EintrÃ¤ge in {current_folder} nicht im Ordner"
                    )

            result["statistics"]["missing_in_cache"].extend(
                f"{current_folder}/{f}" for f in missing_in_cache
            )
            result["statistics"]["missing_in_folder"].extend(
                f"{current_folder}/{f}" for f in missing_in_folder
            )

            # Update progress
            await update_progress(
                f"Ordner verarbeitet: {folder_idx}/{total_folders}",
                int((folder_idx / total_folders) * 100)
            )

        # Abschlussbericht
        total_inconsistencies = len(result["statistics"]["missing_in_cache"]) + \
                                len(result["statistics"]["missing_in_folder"])

        if total_inconsistencies == 0:
            await update_progress_text("âœ… Lokaler Cache ist vollstÃ¤ndig konsistent!")
        else:
            await update_progress_text(
                f"âš ï¸ {total_inconsistencies} Inkonsistenzen gefunden:\n"
                f"   â€¢ {len(result['statistics']['missing_in_cache'])} Dateien nicht im Cache\n"
                f"   â€¢ {len(result['statistics']['missing_in_folder'])} Cache-EintrÃ¤ge nicht im Ordner"
            )

    except Exception as e:
        await update_progress_text(f"âŒ Fehler bei der Cache-ÃœberprÃ¼fung: {e}")
    finally:
        await stop_progress()

    return result


async def delete_duplicates_in_gdrive_folder(service, folder_id: str) -> None:
    """
    LÃ¶scht doppelte Dateien (basierend auf MD5 Hash) in einem Google Drive Ordner.

    Args:
        service: Google Drive Service Objekt
        folder_id: ID des zu prÃ¼fenden Ordners
    """
    try:
        await update_progress_text("ðŸ” Suche nach Dateien im Ordner...")

        # Alle Dateien im Ordner abrufen
        files = []
        page_token = None
        while True:
            response = service.files().list(
                q=f"'{folder_id}' in parents and trashed=false",
                spaces='drive',
                fields='nextPageToken, files(id, name, md5Checksum)',
                pageSize=Settings.PAGESIZE,
                pageToken=page_token
            ).execute()

            files.extend(response.get('files', []))
            page_token = response.get('nextPageToken')
            if not page_token:
                break

        # MD5 Hashes gruppieren
        md5_groups = {}
        for file in files:
            md5 = file.get('md5Checksum')
            if md5:
                if md5 not in md5_groups:
                    md5_groups[md5] = []
                md5_groups[md5].append(file)

        # Duplikate finden und lÃ¶schen
        deleted_count = 0
        for md5, file_group in md5_groups.items():
            if len(file_group) > 1:
                # Behalte die erste Datei, lÃ¶sche den Rest
                original = file_group[0]
                duplicates = file_group[1:]

                await update_progress_text(f"ðŸ” Gefunden: {original['name']} hat {len(duplicates)} Duplikate")

                for dup in duplicates:
                    try:
                        service.files().delete(fileId=dup['id']).execute()
                        deleted_count += 1
                        await update_progress_text(f"ðŸ—‘ï¸ GelÃ¶scht: {dup['name']}")
                    except Exception as e:
                        await update_progress_text(f"âŒ Fehler beim LÃ¶schen von {dup['name']}: {str(e)}")

        await update_progress_text(f"âœ… Abgeschlossen: {deleted_count} Duplikate gelÃ¶scht")

    except Exception as e:
        await update_progress_text(f"âŒ Fehler: {str(e)}")


async def move_duplicates_in_folder(folder_path: str) -> dict[str, list[Path]]:
    """
    Verschiebt doppelte Bilddateien in einem lokalen Ordner und seinen Unterordnern
    in den TEMP_DIR_PATH Ordner.
    """
    files_by_md5: Dict[str, List[Path]] = {}

    try:
        base_folder = Path(folder_path)
        folder_name = base_folder.name
        await update_progress_text(f"[{folder_name}] ðŸ” Initialisiere...")

        # Temp-Ordner vorbereiten
        Settings.TEMP_DIR_PATH.mkdir(parents=True, exist_ok=True)

        # Sammle alle Unterordner und Dateien vorab fÃ¼r bessere Fortschrittsanzeige
        folders: Dict[str, Path] = {folder_name: base_folder}
        total_files = 0
        files_to_process = []

        await update_progress_text(f"[{folder_name}] ðŸ“‚ Sammle Dateien...")

        # Sammle erst alle Ordner und zÃ¤hle Dateien
        for subdir in base_folder.rglob("*"):
            if subdir.is_dir():
                folders[subdir.name] = subdir
                for file_path in subdir.iterdir():
                    if file_path.is_file() and any(
                            file_path.name.lower().endswith(ext) for ext in Settings.IMAGE_EXTENSIONS):
                        total_files += 1
                        files_to_process.append(file_path)

        # Hauptordner auch durchsuchen
        for file_path in base_folder.iterdir():
            if file_path.is_file() and any(file_path.name.lower().endswith(ext) for ext in Settings.IMAGE_EXTENSIONS):
                total_files += 1
                files_to_process.append(file_path)

        await update_progress_text(f"[{folder_name}] ðŸ“‚ {len(folders)} Ordner, {total_files} Dateien gefunden")

        # Verarbeite Dateien
        processed_files = 0
        last_progress = 0

        for file_path in files_to_process:
            try:
                # Dateinamen Ã¼berprÃ¼fen und ggf. umbenennen
                original_name = file_path.name
                sanitized_name = sanitize_filename(original_name)

                if original_name != sanitized_name:
                    try:
                        new_path = file_path.parent / sanitized_name
                        file_path.rename(new_path)
                        file_path = new_path
                    except Exception as e:
                        await update_progress_text(
                            f"[{folder_name}] âŒ Fehler beim Umbenennen von {original_name}: {str(e)}")
                        continue

                # MD5 Hash berechnen
                with open(file_path, 'rb') as f:
                    md5 = hashlib.md5(f.read()).hexdigest()
                    if md5 not in files_by_md5:
                        files_by_md5[md5] = []
                    files_by_md5[md5].append(file_path)

                processed_files += 1
                current_progress = int((processed_files / total_files) * 100)

                # Update nur bei signifikanter Ã„nderung
                if current_progress - last_progress >= 5:
                    await update_progress_text(
                        f"[{folder_name}] ðŸ” Analysiere Dateien: {processed_files}/{total_files} ({current_progress}%)",
                        ctime=0.01)
                    last_progress = current_progress

            except Exception as e:
                await update_progress_text(
                    f"[{folder_name}] âŒ Fehler bei {file_path.name}: {str(e)}")

        # Duplikate in temp Ordner verschieben
        duplicates_found = sum(len(files) - 1 for files in files_by_md5.values() if len(files) > 1)
        await update_progress_text(f"[{folder_name}] ðŸŽ¯ {duplicates_found} Duplikate gefunden")

        moved_count = 0
        for md5, file_list in files_by_md5.items():
            if len(file_list) > 1:
                original = file_list[0]
                duplicates = file_list[1:]

                for dup in duplicates:
                    try:
                        target_path = Settings.TEMP_DIR_PATH / dup.name
                        if target_path.exists():
                            base_name = target_path.stem
                            suffix = target_path.suffix
                            counter = 1
                            while target_path.exists():
                                target_path = Settings.TEMP_DIR_PATH / f"{base_name}_{counter}{suffix}"
                                counter += 1

                        dup.rename(target_path)
                        moved_count += 1

                        # Update nur bei jedem 5. Move oder bei Vielfachen von 10
                        if moved_count % 5 == 0 or moved_count % 10 == 0:
                            await update_progress_text(
                                f"[{folder_name}] ðŸ“¦ {moved_count}/{duplicates_found} Duplikate verschoben",
                                ctime=0.01)
                    except Exception as e:
                        await update_progress_text(
                            f"[{folder_name}] âŒ Fehler beim Verschieben von {dup.name}: {str(e)}",
                            ctime=0.01)

        await update_progress_text(
            f"[{folder_name}] âœ… Abgeschlossen: {moved_count} von {duplicates_found} Duplikaten verschoben")

    except Exception as e:
        await update_progress_text(f"[{folder_path}] âŒ Fehler: {str(e)}")

    return files_by_md5


async def download_file(service, file_id, local_path):
    request = service.files().get_media(fileId=file_id)
    with open(local_path, 'wb') as f:
        downloader = MediaIoBaseDownload(f, request)
        done = False
        while not done:
            status, done = downloader.next_chunk()


async def upload_file_to_gdrive(service, file_path: Path, target_folder_id: str) -> bool:
    """
    LÃ¤dt eine Datei in Google Drive hoch.

    Args:
        service: Google Drive Service Objekt
        file_path: Path Objekt zur lokalen Datei
        target_folder_id: ID des Zielordners in Google Drive

    Returns:
        bool: True wenn Upload erfolgreich, False bei Fehler
    """
    try:
        await update_progress_text(f"â¬†ï¸ Lade {file_path.name} hoch...")

        # Metadata fÃ¼r die Datei
        file_metadata = {
            'name': file_path.name,
            'parents': [target_folder_id]
        }

        # MediaFileUpload Objekt erstellen
        media = MediaFileUpload(
            str(file_path),
            mimetype='image/*',
            resumable=True
        )

        # Upload mit Progress-Updates
        request = service.files().create(
            body=file_metadata,
            media_body=media,
            fields='id'
        )

        response = None
        while response is None:
            status, response = request.next_chunk()
            if status:
                await update_progress(
                    f"â¬†ï¸ Upload von {file_path.name}",
                    int(status.progress() * 100)
                )

        await update_progress_text(f"âœ… {file_path.name} erfolgreich hochgeladen (ID: {response.get('id')})")
        return True

    except Exception as e:
        await update_progress_text(f"âŒ Fehler beim Hochladen von {file_path.name}: {e}")
        return False


async def dd(
        service: object,
        files_by_md5: Dict[str, List[Path]] | None,
        md5_groups_gdrive: dict[Any, Any] | None) -> None:
    """

    :type service: object
    """
    # Speichern der aktuellen Daten
    cache_dir = Path(Settings.IMAGE_FILE_CACHE_DIR).parent
    cache_dir.mkdir(exist_ok=True)

    files_cache = cache_dir / "files_by_md5.json"
    gdrive_cache = cache_dir / "md5_groups_gdrive.json"

    # Laden der Caches, wenn Parameter None sind
    if files_by_md5 is None:
        try:
            with files_cache.open('r') as f:
                # Konvertiere Pfad-Strings zurÃ¼ck zu Path-Objekten
                data = json.load(f)
                files_by_md5 = {
                    md5: [Path(p) for p in paths]
                    for md5, paths in data.items()
                }
        except (FileNotFoundError, json.JSONDecodeError):
            files_by_md5 = {}

    if md5_groups_gdrive is None:
        try:
            with gdrive_cache.open('r') as f:
                md5_groups_gdrive = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            md5_groups_gdrive = {}

    if not files_by_md5 or not md5_groups_gdrive:
        return

    try:
        # Konvertiere Path-Objekte zu Strings fÃ¼r JSON-Serialisierung
        serializable_files = {
            md5: [str(p) for p in paths]
            for md5, paths in files_by_md5.items()
        }
        with files_cache.open('w') as f:
            json.dump(serializable_files, f, indent=2)

        with gdrive_cache.open('w') as f:
            json.dump(md5_groups_gdrive, f, indent=2)
    except Exception as e:
        await update_progress_text(f"âŒ Fehler beim Speichern der Caches: {str(e)}")

    folder_name = "Alle"
    # Nach dem Verschieben der Duplikate, Vergleich mit GDrive durchfÃ¼hren
    await update_progress_text(f"[{folder_name}] ðŸ”„ Vergleiche mit GDrive-Hashes...")

    # Finde Dateien die nur lokal existieren
    local_only_hashes = set(files_by_md5.keys()) - set(md5_groups_gdrive.keys())
    if local_only_hashes:
        await update_progress_text(f"[{folder_name}] ðŸ“Œ {len(local_only_hashes)} Dateien nur lokal gefunden")

        for md5 in local_only_hashes:
            local_files = files_by_md5[md5]
            for file_path in local_files:
                try:
                    # Bestimme den Zielordner aus dem Elternverzeichnis der Datei
                    target_folder = file_path.parent.name
                    target_folder_id = folder_id_by_name(target_folder)

                    if not target_folder_id:
                        await update_progress_text(
                            f"[{folder_name}] âš ï¸ Keine Folder-ID fÃ¼r: {target_folder}")
                        continue

                    # Upload durchfÃ¼hren
                    success = await upload_file_to_gdrive(
                        service,
                        file_path,
                        target_folder_id
                    )

                    if success:
                        await update_progress_text(
                            f"[{folder_name}] ðŸ“¤ Hochgeladen: {file_path.name} â†’ {target_folder}")

                except Exception as e:
                    await update_progress_text(
                        f"[{folder_name}] âŒ Upload-Fehler bei {file_path.name}: {str(e)}")
                    continue

    # Finde Dateien die nur in GDrive existieren
    gdrive_only_hashes = set(md5_groups_gdrive.keys()) - set(files_by_md5.keys())
    if gdrive_only_hashes:
        await update_progress_text(f"[{folder_name}] â˜ï¸ {len(gdrive_only_hashes)} Dateien nur in GDrive gefunden")
        for md5 in gdrive_only_hashes:
            gdrive_files = md5_groups_gdrive[md5]
            for file_info in gdrive_files:
                name = file_info.get('name');
                await update_progress_text(f"[{folder_name}] â˜ï¸ Nur GDrive: {name}")
                try:
                    file_id = file_info.get('id')
                    local_file = Path(Settings.IMAGE_FILE_CACHE_DIR) / "recheck" / name
                    await download_file(service, file_id, local_file)
                    if local_file.exists():
                        await update_progress_text(f"ðŸ“¥ Heruntergeladen: {local_file}")
                except Exception as e:
                    await update_progress_text(f"âŒ Download-Fehler bei {name}: {e}")

    # Finde Unterschiede bei Dateien die in beiden existieren
    common_hashes = set(files_by_md5.keys()) & set(md5_groups_gdrive.keys())
    name_mismatches = []

    for md5 in common_hashes:
        local_names = {f.name for f in files_by_md5[md5]}
        gdrive_names = {f.get('name', '') for f in md5_groups_gdrive[md5]}

        # PrÃ¼fe auf Namensunterschiede
        if local_names != gdrive_names:
            name_mismatches.append({
                'md5': md5,
                'local_names': local_names,
                'gdrive_names': gdrive_names
            })

    if name_mismatches:
        await update_progress_text(
            f"[{folder_name}] âš ï¸ {len(name_mismatches)} Dateien mit unterschiedlichen Namen gefunden")
        for mismatch in name_mismatches:
            try:
                if not mismatch['local_names']:
                    await update_progress_text(
                        f"[{folder_name}] âš ï¸ Keine lokalen Namen fÃ¼r MD5: {mismatch['md5']}",
                        ctime=0.01)
                    continue

                local_name = mismatch['local_names'].pop()
                sanitized_name = sanitize_filename(local_name)

                # Hier fehlt die GDrive File ID - wir mÃ¼ssen sie aus md5_groups_gdrive holen
                gdrive_files = md5_groups_gdrive[mismatch['md5']]
                for gdrive_file in gdrive_files:
                    service.files().update(
                        fileId=gdrive_file['id'],
                        body={'name': sanitized_name}
                    ).execute()
                    await update_progress_text(
                        f"[{folder_name}] âœï¸ Umbenannt: {gdrive_file['name']} â†’ {sanitized_name}")

            except Exception as e:
                await update_progress_text(
                    f"[{folder_name}] âŒ Fehler beim Umbenennen (MD5: {mismatch['md5']}): {str(e)}")
                continue

    summary = (
        f"[{folder_name}] ðŸ“Š Zusammenfassung:\n"
        f"- {len(local_only_hashes)} Dateien nur lokal\n"
        f"- {len(gdrive_only_hashes)} Dateien nur in GDrive\n"
        f"- {len(name_mismatches)} Dateien mit Namensunterschieden"
    )


async def move_duplicates_to_temp(
        service: object,
        md5_groups: dict,
        temp_folder_id: str,
        folder_name: str
) -> int:
    """
    Verschiebt Duplikate in einen temporÃ¤ren Ordner.

    Args:
        service: Google Drive Service
        md5_groups: Dictionary mit MD5-Hash als Key und Liste von Dateien als Value
        temp_folder_id: ID des temporÃ¤ren Ordners
        folder_name: Name des aktuellen Ordners fÃ¼r Logging

    Returns:
        int: Anzahl der verschobenen Dateien
    """
    moved_count = 0
    total_duplicates = sum(len(file_group) - 1
                           for file_group in md5_groups.values()
                           if len(file_group) > 1)

    if total_duplicates == 0:
        return 0

    await update_progress(f"[{folder_name}] ðŸ” Verarbeite Duplikate", 0)

    current_count = 0
    for md5, file_group in md5_groups.items():
        if len(file_group) > 1:
            original = file_group[0]
            duplicates = file_group[1:]

            for dup in duplicates:
                try:
                    service.files().update(
                        fileId=dup['id'],
                        addParents=temp_folder_id,
                        removeParents=dup['parents'][0]
                    ).execute()
                    moved_count += 1
                    current_count += 1

                    # Update nur alle 5% oder bei jedem 10ten File
                    if current_count % 10 == 0 or (current_count / total_duplicates) * 100 % 5 == 0:
                        progress = int((current_count / total_duplicates) * 100)
                        await update_progress(
                            f"[{folder_name}] ðŸ“¦ Verschiebe Duplikate ({current_count}/{total_duplicates})",
                            progress
                        )

                except Exception as e:
                    await update_progress_text(
                        f"[{folder_name}] âŒ Fehler beim Verschieben von {dup['name']}: {str(e)}")
                    current_count += 1
                    continue

    await update_progress(f"[{folder_name}] âœ… {moved_count} Duplikate verschoben", 100)
    return moved_count


async def move_duplicates_in_gdrive_folder(service, folder_id: str) -> dict[Any, Any]:
    """
    Verschiebt doppelte Bilddateien in einem Google Drive Ordner und seinen Unterordnern
    in einen 'temp' Ordner.
    """
    md5_groups = {}

    try:
        folder_name = service.files().get(fileId=folder_id, fields="name").execute().get("name", "Unbekannt")
        await update_progress_text(f"[{folder_name}] ðŸ” Initialisiere Suche nach Duplikaten...")

        # Hole temp Ordner ID
        temp_folder_id = folder_id_by_name("temp")
        await update_progress_text(f"[{folder_name}] ðŸ“ Temp Ordner ID: {temp_folder_id}")

        # Sammle erst alle Ordner
        name_to_id = {}
        id_to_name = {}
        collect_all_folders(service, folder_id, name_to_id, id_to_name)

        # Alle Ordner-IDs sammeln (inkl. Ursprungsordner)
        folder_ids = list(id_to_name.keys())
        folder_ids.append(folder_id)

        files = []
        for current_folder_id in folder_ids:
            current_folder_name = id_to_name.get(current_folder_id, "Ursprungsordner")
            try:
                await update_progress_text(f"[{folder_name}/{current_folder_name}] ðŸ“‚ Verarbeite Ordner...")

                # Query fÃ¼r aktuellen Ordner
                query = f"'{current_folder_id}' in parents and trashed=false"

                # Dateien fÃ¼r aktuellen Ordner abrufen
                page_token = None
                while True:
                    response = service.files().list(
                        q=query,
                        spaces='drive',
                        fields='nextPageToken, files(id, name, md5Checksum, parents)',
                        pageSize=Settings.PAGESIZE,
                        pageToken=page_token
                    ).execute()

                    batch = response.get('files', [])
                    files.extend(batch)
                    await update_progress_text(
                        f"[{folder_name}/{current_folder_name}] ðŸ“‚ {len(files)} Dateien gefunden...")

                    page_token = response.get('nextPageToken')
                    if not page_token:
                        break
            except Exception as e:
                await update_progress_text(f"[{folder_name}/{current_folder_name}] âŒ Fehler beim Verarbeiten: {str(e)}")

        # Zuerst alle Dateinamen Ã¼berprÃ¼fen und ggf. umbenennen
        for file in files:
            if not any(file.get('name', '').lower().endswith(ext) for ext in Settings.IMAGE_EXTENSIONS):
                continue

            original_name = file.get('name', '')
            sanitized_name = sanitize_filename(original_name)

            if original_name != sanitized_name:
                try:
                    service.files().update(
                        fileId=file['id'],
                        body={'name': sanitized_name}
                    ).execute()
                    await update_progress_text(f"[{folder_name}] âœï¸ Umbenannt: {original_name} â†’ {sanitized_name}")
                    file['name'] = sanitized_name
                except Exception as e:
                    await update_progress_text(
                        f"[{folder_name}] âŒ Fehler beim Umbenennen von {original_name}: {str(e)}")

        # Duplikate nach MD5 gruppieren
        for file in files:
            if not any(file.get('name', '').lower().endswith(ext) for ext in Settings.IMAGE_EXTENSIONS):
                continue

            md5 = file.get('md5Checksum')
            if md5:
                if md5 not in md5_groups:
                    md5_groups[md5] = []
                md5_groups[md5].append(file)

        # Verschiebe Duplikate
        moved_count = await move_duplicates_to_temp(
            service=service,
            md5_groups=md5_groups,
            temp_folder_id=temp_folder_id,
            folder_name=folder_name
        )

        await update_progress_text(
            f"[{folder_name}] âœ… Abgeschlossen: {moved_count} Duplikate in temp Ordner verschoben")

    except Exception as e:
        await update_progress_text(f"[{folder_name}] âŒ Fehler: {str(e)}")

    return md5_groups


async def update_local_hash(directory: Path, file_name: str, file_md5: str, addordel: bool) -> None:
    """
    Aktualisiert die lokale Hash-Datei in einem Verzeichnis.

    Args:
        directory: Verzeichnispfad
        file_name: Name der Datei
        file_md5: MD5-Hash der Datei
        addordel: True zum HinzufÃ¼gen, False zum Entfernen des Hashes
    """
    try:
        hash_path = directory / Settings.GALLERY_HASH_FILE
        local_hashes = {}

        if hash_path.exists():
            try:
                with hash_path.open('r', encoding='utf-8') as f:
                    local_hashes = json.load(f)
            except json.JSONDecodeError as e:
                await update_progress_text(f"âš ï¸ Hash-Datei beschÃ¤digt: {e}")
                backup_path = hash_path.with_suffix('.bak')
                hash_path.rename(backup_path)

        old_hash = local_hashes.get(file_name)

        if addordel:
            if old_hash != file_md5:
                local_hashes[file_name] = file_md5
                await update_progress_text(f"âž• Hash hinzugefÃ¼gt - {file_name}: {file_md5}")
        else:
            if file_name in local_hashes:
                del local_hashes[file_name]
                await update_progress_text(f"âž– Hash entfernt - {file_name}")

        await save_simple_hashes(local_hashes, hash_path)

    except Exception as e:
        await update_progress_text(f"âŒ Hash-Update fehlgeschlagen - {file_name}: {str(e)}")
        raise


def p5():
    Settings.DB_PATH = '../../gallery_local.db'
    Settings.TEMP_DIR_PATH = Path("../../cache/temp")
    Settings.IMAGE_FILE_CACHE_DIR = "../../cache/imagefiles"
    Settings.TEXT_FILE_CACHE_DIR = "../../cache/textfiles"
    Settings.PAIR_CACHE_PATH = "../../cache/pair_cache_local.json"
    SettingsGdrive.GDRIVE_FOLDERS_PKL = Path("../../cache/gdrive_folders.pkl")

    for idx, kat in enumerate(Settings.kategorien, 1):
        folder_name = kat["key"]
        result = asyncio.run(verify_file_cache_consistency(Path(Settings.IMAGE_FILE_CACHE_DIR), folder_name))
        if len(result['inconsistencies']) > 0:
            # Die Ergebnisse auswerten
            print(f"GeprÃ¼fte Dateien: {result['statistics']['total_files']}")
            print(f"Cache-EintrÃ¤ge: {result['statistics']['cached_files']}")
            print(f"Inkonsistente Ordner: {len(result['inconsistencies'])}")

    result = asyncio.run(verify_file_cache_consistency(Path(Settings.TEXT_FILE_CACHE_DIR).parent, "textfiles"))
    if len(result['inconsistencies']) > 0:
        # Die Ergebnisse auswerten
        print(f"GeprÃ¼fte Dateien: {result['statistics']['total_files']}")
        print(f"Cache-EintrÃ¤ge: {result['statistics']['cached_files']}")
        print(f"Inkonsistente Ordner: {len(result['inconsistencies'])}")


async def mache_alles(service):
    await init_progress_state()
    progress_state["running"] = True
    await update_progress_text("ðŸ”„ Starting duplicate detection")

    await update_all_local_hashes()
    await update_all_gdrive_hashes(service)
    await dd(service, None, None)

    files_by_md5 = await  move_duplicates_in_folder(Settings.IMAGE_FILE_CACHE_DIR)

    md5_groups_gdrive = await move_duplicates_in_gdrive_folder(service, folder_id_by_name("imagefiles"))

    await dd(service, files_by_md5, md5_groups_gdrive)
    await update_all_local_hashes()
    await update_all_gdrive_hashes(service)


def p6():
    Settings.DB_PATH = '../../gallery_local.db'
    Settings.TEMP_DIR_PATH = Path("../../cache/temp")
    Settings.IMAGE_FILE_CACHE_DIR = "../../cache/imagefiles"
    Settings.TEXT_FILE_CACHE_DIR = "../../cache/textfiles"
    Settings.PAIR_CACHE_PATH = "../../cache/pair_cache_local.json"
    SettingsGdrive.GDRIVE_FOLDERS_PKL = Path("../../cache/gdrive_folders.pkl")

    service = load_drive_service_token(os.path.abspath(os.path.join("../../secrets", "token.json")))

    asyncio.run(mache_alles(service))


def p7():
    Settings.DB_PATH = '../../gallery_local.db'
    Settings.TEMP_DIR_PATH = Path("../../cache/temp")
    Settings.IMAGE_FILE_CACHE_DIR = "../../cache/imagefiles"
    Settings.TEXT_FILE_CACHE_DIR = "../../cache/textfiles"
    Settings.PAIR_CACHE_PATH = "../../cache/pair_cache_local.json"
    SettingsGdrive.GDRIVE_FOLDERS_PKL = Path("../../cache/gdrive_folders.pkl")

    service = load_drive_service_token(os.path.abspath(os.path.join("../../secrets", "token.json")))

    asyncio.run(reloadcache_progress(service, "recheck"))


if __name__ == "__main__":
    p6()
