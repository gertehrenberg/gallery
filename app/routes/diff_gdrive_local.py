# Refactored diff_gdrive_local.py with full logging
import asyncio
import io
import os
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from uuid import uuid4

from fastapi import APIRouter, Request
from fastapi.responses import HTMLResponse, RedirectResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload
from ..config import Settings, UserType
from ..config_gdrive import calculate_md5, folder_id_by_name
from ..config_gdrive import sanitize_filename
from ..routes.auth import load_drive_service
from ..utils.logger_config import setup_logger

VERSION = 201
logger = setup_logger(__name__)

router = APIRouter()
templates = Jinja2Templates(directory=os.path.join(os.path.dirname(__file__), "../templates"))

PROGRESS = {"status": "Bereit", "progress": 0, "details": {"status": "Bereit", "progress": 0}}
PROGRESS_LOCK = asyncio.Lock()
LOCAL_BASE = Settings.IMAGE_FILE_CACHE_DIR
EXECUTOR = ThreadPoolExecutor(max_workers=8)
GLOBAL_MD5_INDEX = {}  # md5 -> {"local": [...], "gdrive": [...]}

SCAN_CACHE = {
    "categories": [],
    "invalid_md5": [],
    "invalid_names": [],
    "filename_collisions": []
}

UID_CACHE = {}
# Globaler Fortschritt f√ºr Dateiscans
processed_files = 0
total_files = 0


async def prepare_total_file_count(categories):
    """Z√§hlt alle Dateien in allen Kategorien (local + gdrive)."""
    total_files = 0
    for cat in categories:
        # lokale Dateien
        lfiles = await local_list_folder(cat)
        total_files += len(lfiles)

        # gdrive Dateien
        gfiles = await gdrive_list_folder(cat)
        total_files += len(gfiles)

    return total_files


async def local_list_folder(folder_name: str):
    folder_path = os.path.join(LOCAL_BASE, folder_name)
    if not os.path.isdir(folder_path):
        return []

    result = []
    for filename in os.listdir(folder_path):
        full = os.path.join(folder_path, filename)
        if os.path.isfile(full):
            result.append({
                "id": f"{folder_name}/{filename}",
                "folder": folder_name,
                "name": filename,
                "path": full,
                "size": os.path.getsize(full),
            })
    return result


def compute_md5_file(path: str):
    return calculate_md5(Path(path))


async def set_progress(status, progress, detail_status=None, detail_progress=None):
    async with PROGRESS_LOCK:
        PROGRESS["status"] = status
        PROGRESS["progress"] = progress
        logger.info(f"set_progress: {status} {progress}")
        if detail_status is not None:
            PROGRESS["details"]["status"] = detail_status
        if detail_progress is not None:
            PROGRESS["details"]["progress"] = detail_progress


async def set_progress_detail(detail_status=None, detail_progress=None):
    async with PROGRESS_LOCK:
        if detail_status is not None:
            PROGRESS["details"]["status"] = detail_status
        if detail_progress is not None:
            PROGRESS["details"]["progress"] = detail_progress


async def update_file_progress(processed_files, total_files, source):
    """Aktualisiert den Detail-Fortschritt je gescannte Datei."""
    percent = int(processed_files / total_files * 100)
    await set_progress_detail(
        detail_status=f"Scanne {source} ({processed_files}/{total_files})",
        detail_progress=percent
    )


def reset_progress():
    PROGRESS["status"] = "Bereit"
    PROGRESS["progress"] = 0
    PROGRESS["details"] = {"status": "Bereit", "progress": 0}


async def gdrive_list_folder(folder_name: str):
    folder_id = folder_id_by_name(folder_name)
    if not folder_id:
        logger.warning(f"‚ö†Ô∏è Kein Folder ID f√ºr Kategorie {folder_name}")
        return []

    # Globalen Status setzen
    await set_progress_detail(
        detail_status=f"{folder_name}: lade GDrive Dateien‚Ä¶",
        detail_progress=0
    )

    service = load_drive_service()

    query = (
        f"'{folder_id}' in parents "
        f"and trashed = false "
        f"and mimeType != 'application/vnd.google-apps.folder' "
        f"and mimeType != 'application/vnd.google-apps.shortcut'"
    )

    # Ergebniscontainer
    collected_files = []

    # Das Paging l√§uft im Threadpool, Fortschritt aber im Eventloop!
    async def fetch_paged():
        token = None
        processed = 0
        estimated_total = 200  # Startsch√§tzung

        while True:
            # Blockierende Google-API im Thread-Executor
            resp = await asyncio.get_running_loop().run_in_executor(
                EXECUTOR,
                lambda: service.files().list(
                    q=query,
                    spaces="drive",
                    fields="nextPageToken, files(id,name,md5Checksum,size,parents)",
                    supportsAllDrives=True,
                    includeItemsFromAllDrives=True,
                    pageToken=token,
                    pageSize=Settings.PAGESIZE,
                ).execute()
            )

            page_files = resp.get("files", [])
            collected_files.extend(page_files)

            processed = len(collected_files)

            # Fortschritt aktualisieren
            await update_file_progress(
                processed_files=processed,
                total_files=estimated_total,
                source=f"GDrive {folder_name}"
            )

            # Estimate erh√∂hen solange Daten kommen
            estimated_total = max(estimated_total, processed + Settings.PAGESIZE)

            token = resp.get("nextPageToken")
            if not token:
                break

        # Finalen Wert setzen
        await update_file_progress(
            processed_files=processed,
            total_files=processed,
            source=f"GDrive {folder_name}"
        )

    # Paging starten
    await fetch_paged()

    logger.info(f"üìÅ GDrive Folder {folder_name}: {len(collected_files)} Dateien")
    return collected_files


async def find_case_duplicates(folder_name: str, idx: int, total: int):
    global processed_files, total_files

    logger.info(f"üîç Scanne Kategorie: {folder_name}")

    await set_progress(
        f"Kategorie {idx + 1}/{total}: {folder_name}",
        PROGRESS["progress"],  # <-- Hauptfortschritt unver√§ndert lassen
        detail_status="Initialisiere GDrive‚Ä¶",
        detail_progress=int(processed_files / total_files * 100) if total_files else 0
    )

    # üî• Sofort sichtbar, bevor erster API-Call blockiert
    await set_progress_detail(
        detail_status=f"{folder_name}: lade GDrive Dateien‚Ä¶",
        detail_progress=int(processed_files / total_files * 100) if total_files else 0
    )

    # ----------------------------------------------------------
    # üìÅ GDRIVE SCAN
    # ----------------------------------------------------------
    gfiles = await gdrive_list_folder(folder_name)
    g_insert_before = sum(len(v.get("gdrive", [])) for v in GLOBAL_MD5_INDEX.values())

    for f in gfiles:
        if f["name"].lower().endswith(".json"):
            continue
        md5 = f.get("md5Checksum")
        if not md5:
            continue

        folder_id = (f.get("parents") or ["?"])[0]

        clean = sanitize_filename(f["name"])
        invalid_name = (clean != f["name"])

        GLOBAL_MD5_INDEX.setdefault(md5, {"local": [], "gdrive": []})
        GLOBAL_MD5_INDEX[md5]["gdrive"].append({
            "folder": folder_name,
            "folder_id": folder_id,
            "name": f["name"],
            "id": f["id"],
            "sanitized_name": clean,
            "is_invalid_name": invalid_name
        })

        if invalid_name:
            SCAN_CACHE["invalid_names"].append({
                "source": "gdrive",
                "folder": folder_name,
                "orig_name": f["name"],
                "clean_name": clean,
                "id": f["id"],
                "md5": md5,
            })

        # Fortschritt pro Datei
        processed_files += 1
        await set_progress_detail(
            detail_status=f"Scanne GDrive ({processed_files}/{total_files})",
            detail_progress=int(processed_files / total_files * 100)
        )

    g_insert_after = sum(len(v.get("gdrive", [])) for v in GLOBAL_MD5_INDEX.values())
    logger.info(f"üì• GDRIVE Insert: vorher={g_insert_before}, nachher={g_insert_after}")

    # ----------------------------------------------------------
    # üñ• LOCAL SCAN
    # ----------------------------------------------------------
    lfiles = await local_list_folder(folder_name)
    l_insert_before = sum(len(v.get("local", [])) for v in GLOBAL_MD5_INDEX.values())

    loop = asyncio.get_running_loop()

    # Vor Beginn sichtbar machen
    await set_progress_detail(f"{folder_name}: scanne lokale Dateien‚Ä¶")

    for lf in lfiles:
        if lf["name"].lower().endswith(".json"):
            continue

        md5 = await loop.run_in_executor(EXECUTOR, compute_md5_file, lf["path"])

        clean = sanitize_filename(lf["name"])
        invalid_name = (clean != lf["name"])

        GLOBAL_MD5_INDEX.setdefault(md5, {"local": [], "gdrive": []})
        GLOBAL_MD5_INDEX[md5]["local"].append({
            "folder": folder_name,
            "path": lf["path"],
            "name": lf["name"],
            "sanitized_name": clean,
            "is_invalid_name": invalid_name
        })

        if invalid_name:
            uid = uuid4().hex
            UID_CACHE[uid] = {
                "source": "local",
                "folder": folder_name,
                "path": lf["path"],
                "orig_name": lf["name"],
                "clean_name": clean,
                "md5": md5,
            }
            SCAN_CACHE["invalid_names"].append({
                "source": "local",
                "folder": folder_name,
                "orig_name": lf["name"],
                "clean_name": clean,
                "path": lf["path"],
                "md5": md5,
                "uid": uid,
            })

        processed_files += 1
        await set_progress_detail(
            detail_status=f"Scanne Local ({processed_files}/{total_files})",
            detail_progress=int(processed_files / total_files * 100)
        )

    l_insert_after = sum(len(v.get("local", [])) for v in GLOBAL_MD5_INDEX.values())
    logger.info(f"üì• LOCAL Insert: vorher={l_insert_before}, nachher={l_insert_after}")

    # Kollisionen
    await filename_collision(folder_name)

    return {"folder": folder_name, "results": []}


async def filename_collision(folder_name: str):
    # -------------------------------------------------------
    # NEU: Dateinamen-Kollisionen erkennen
    # -------------------------------------------------------
    logger.info(f"üîç Starte Filename-Kollisionsscan f√ºr Ordner: {folder_name}")

    name_map = {}  # name -> list of (md5, source, entry)

    # 1) lokale Dateien sammeln
    for md5, entry in GLOBAL_MD5_INDEX.items():
        for item in entry["local"]:
            name_map.setdefault(item["name"], [])
            name_map[item["name"]].append({
                "md5": md5,
                "source": "local",
                "entry": item
            })

    logger.info(f"üìÅ Lokale Dateien gesammelt: {sum(len(v) for v in name_map.values())}")

    # 2) gdrive Dateien sammeln
    for md5, entry in GLOBAL_MD5_INDEX.items():
        for item in entry["gdrive"]:
            name_map.setdefault(item["name"], [])
            name_map[item["name"]].append({
                "md5": md5,
                "source": "gdrive",
                "entry": item
            })

    logger.info(
        f"üìÅ Lokale + GDrive-Dateien total gesammelt f√ºr Namensmapping: "
        f"{sum(len(v) for v in name_map.values())}"
    )

    # 3) Kollisionen finden
    for filename, items in name_map.items():
        md5_values = {x["md5"] for x in items}

        if len(md5_values) > 1:
            # <<< HIER WICHTIG: md5 mit in die Eintr√§ge aufnehmen >>>
            local_entries = [
                {**x["entry"], "md5": x["md5"]}
                for x in items if x["source"] == "local"
            ]

            gdrive_entries = [
                {**x["entry"], "md5": x["md5"]}
                for x in items if x["source"] == "gdrive"
            ]

            logger.warning(
                f"üî• Kollision erkannt: '{filename}' in Ordner '{folder_name}' ‚Üí "
                f"{len(local_entries)} lokal, {len(gdrive_entries)} gdrive, "
                f"MD5s={list(md5_values)}"
            )

            SCAN_CACHE["filename_collisions"].append({
                "folder": folder_name,
                "name": filename,
                "local": local_entries,
                "gdrive": gdrive_entries,
                "md5_list": list(md5_values)
            })


async def run_full_scan():
    global SCAN_CACHE, GLOBAL_MD5_INDEX, processed_files, total_files

    reset_progress()

    GLOBAL_MD5_INDEX.clear()
    UID_CACHE.clear()

    SCAN_CACHE = {
        "categories": [],
        "invalid_md5": [],
        "invalid_names": [],
        "filename_collisions": [],
    }

    Settings._user_type = UserType.ADMIN
    categories = [c["key"] for c in Settings.kategorien() if c["key"] != "XXXX"]
    total_categories = len(categories)

    processed_files = 0
    total_files = 0

    # ----------------------------------------------------------
    # PHASE 1 ‚Äî Dateiz√§hlung (0‚Äì10%)
    # ----------------------------------------------------------
    await set_progress(
        "Z√§hle Dateien‚Ä¶",
        1,
        detail_status="Vorbereitung‚Ä¶",
        detail_progress=0
    )

    for idx, cat in enumerate(categories):
        await set_progress(
            f"Z√§hle Dateien ({idx + 1}/{total_categories}): {cat}",
            int((idx / total_categories) * 10),
            detail_status=f"Scanne Ordnerliste f√ºr {cat}",
            detail_progress=0
        )

        lfiles = await local_list_folder(cat)
        total_files += len(lfiles)

        gfiles = await gdrive_list_folder(cat)
        total_files += len(gfiles)

    logger.info(f"üìä Total Files to scan: {total_files}")

    # ----------------------------------------------------------
    # PHASE 2 ‚Äî eigentlicher Scan (10‚Äì90%)
    # ----------------------------------------------------------
    out = []

    for idx, cat in enumerate(categories):
        main_progress = 10 + int((idx / total_categories) * 80)
        await set_progress(
            f"Scanne Kategorie {idx + 1}/{total_categories}: {cat}",
            main_progress,
            detail_status=f"Verarbeite‚Ä¶ {processed_files}/{total_files}",
            detail_progress=int((processed_files / total_files) * 100)
        )

        result = await find_case_duplicates(cat, idx, total_categories)
        out.append(result)

    # ----------------------------------------------------------
    # PHASE 3 ‚Äî MD5 Validierung (90‚Äì100%)
    # ----------------------------------------------------------
    await set_progress(
        "Pr√ºfe MD5-Konsistenz‚Ä¶",
        90,
        detail_status="Analysiere Hash-Anzahl‚Ä¶",
        detail_progress=100
    )

    invalid_md5 = []

    for md5, entry in GLOBAL_MD5_INDEX.items():
        lc = len(entry["local"])
        gc = len(entry["gdrive"])
        if lc != 1 or gc != 1:
            invalid_md5.append({
                "md5": md5,
                "local": entry["local"],
                "gdrive": entry["gdrive"],
                "status": f"{lc}x local, {gc}x gdrive",
            })

    SCAN_CACHE["categories"] = out
    SCAN_CACHE["invalid_md5"] = invalid_md5

    # ----------------------------------------------------------
    # PHASE 4 ‚Äî Fertig
    # ----------------------------------------------------------
    await set_progress("Fertig", 100, "Fertig", 100)
    logger.info("üü¢ Globaler MD5-Scan abgeschlossen")


@router.get("/diff_gdrive_local", response_class=HTMLResponse)
async def diff_gdrive_local(request: Request):
    categories = SCAN_CACHE.get("categories", [])
    invalid_md5 = SCAN_CACHE.get("invalid_md5", [])
    invalid_names = SCAN_CACHE.get("invalid_names", [])
    filename_collisions = SCAN_CACHE.get("filename_collisions", [])

    logger.info(f"categories   : {len(categories)}")
    logger.info(f"invalid_md5  : {len(invalid_md5)}")
    logger.info(f"invalid_names: {len(invalid_names)}")
    logger.info(f"filename_collisions: {len(filename_collisions)}")

    return templates.TemplateResponse(
        "diff_gdrive_local.j2",
        {
            "request": request,
            "categories": categories,
            "invalid_md5": invalid_md5,
            "invalid_names": invalid_names,
            "filename_collisions": filename_collisions,
            "version": VERSION,
        },
    )


@router.post("/diff_gdrive_local_start")
async def diff_gdrive_local_start():
    reset_progress()
    asyncio.get_running_loop().create_task(run_full_scan())
    return JSONResponse({"started": True})


@router.get("/diff_gdrive_local_progress")
async def diff_gdrive_local_progress():
    return JSONResponse(PROGRESS)


@router.get("/diff_gdrive_local_reload")
async def diff_gdrive_local_reload():
    return RedirectResponse("/gallery/diff_gdrive_local")


async def resolve_drive_path(drive, path_segments):
    """
    Gibt die ID des Zielordners zur√ºck.
    Legt NIE Ordner an.
    """
    try:
        parent_id = folder_id_by_name("imagefiles")
    except Exception:
        raise Exception("Drive Basisordner 'imagefiles' nicht gefunden!")

    for seg in path_segments:
        query = (
            f"name='{seg}' and mimeType='application/vnd.google-apps.folder' "
            f"and '{parent_id}' in parents and trashed=false"
        )
        res = drive.files().list(q=query, fields="files(id)").execute()
        folders = res.get("files", [])

        if not folders:
            raise Exception(f"GDrive Unterordner fehlt: {seg}")

        parent_id = folders[0]["id"]

    return parent_id


async def sync_from_gdrive(file_id: str):
    """GDrive ‚Üí Local (Ordner muss existieren!)"""

    drive = load_drive_service()

    try:
        meta = drive.files().get(fileId=file_id, fields="name").execute()
        filename = meta["name"]

        # Ordner √ºber GLOBAL_MD5_INDEX finden
        target_folder = None
        for md5, entry in GLOBAL_MD5_INDEX.items():
            for g in entry["gdrive"]:
                if g["id"] == file_id:
                    target_folder = g["folder"]
                    break

        if target_folder is None:
            raise Exception(f"Kein Ordner f√ºr GDrive-ID {file_id} im MD5-Index!")

        local_target_dir = os.path.join(LOCAL_BASE, target_folder)

        if not os.path.isdir(local_target_dir):
            raise Exception(f"Lokaler Ordner fehlt: {local_target_dir}")

        local_target = os.path.join(local_target_dir, filename)

        # Download
        request_dl = drive.files().get_media(fileId=file_id)
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, request_dl)
        done = False
        while not done:
            _, done = downloader.next_chunk()

        fh.seek(0)

        with open(local_target, "wb") as f:
            f.write(fh.read())

        logger.info(f"‚¨áÔ∏è Sync GDrive ‚Üí Local: {file_id} ‚Üí {local_target}")
        return local_target

    except Exception as e:
        msg = f"Fehler Sync GDrive‚ÜíLocal {file_id}: {e}"
        logger.error(msg)
        return None


async def sync_to_gdrive(local_path: str):
    """Local ‚Üí GDrive (Ordner muss existieren im GDrive!)"""

    drive = load_drive_service()

    try:
        # relativer Pfad unter imagefiles
        rel = os.path.relpath(local_path, LOCAL_BASE)
        parts = rel.split("/")
        folder_parts = parts[:-1]
        filename = parts[-1]

        # GDrive-Ordner nachschlagen
        parent_id = await resolve_drive_path(drive, folder_parts)

        metadata = {
            "name": filename,
            "parents": [parent_id],
        }

        media = MediaFileUpload(local_path, resumable=True)

        new_file = drive.files().create(
            body=metadata,
            media_body=media,
            fields="id"
        ).execute()

        new_id = new_file["id"]
        logger.info(
            f"‚¨ÜÔ∏è Sync Local ‚Üí GDrive: {local_path} ‚Üí {new_id} (Ordner: {'/'.join(folder_parts)})"
        )
        return new_id

    except Exception as e:
        msg = f"Fehler Sync Local‚ÜíGDrive {local_path}: {e}"
        logger.error(msg)
        return None


@router.post("/diff_gdrive_local_delete")
async def diff_gdrive_local_delete(request: Request):
    """
    L√∂scht ausgew√§hlte Dateien (local oder gdrive)
    UND synchronisiert ausgew√§hlte sync_ids.
    OHNE Ordner anzulegen ‚Äì alle Ordner m√ºssen existieren!
    Danach werden betroffene Eintr√§ge aus SCAN_CACHE entfernt.
    """

    form = await request.form()
    drive = load_drive_service()

    rename_ids = form.getlist("rename_ids")
    renamed_local = []
    renamed_gdrive = []

    for source_id in rename_ids:
        # ----------------------------------------------------------
        # A) LOCAL rename (source_id = UID!)
        # ----------------------------------------------------------
        if source_id in UID_CACHE:
            info = UID_CACHE[source_id]

            real_path = info["path"]  # absoluter echter Dateipfad
            new_name = info["clean_name"]
            folder = os.path.dirname(real_path)
            new_path = os.path.join(folder, new_name)

            try:
                os.rename(real_path, new_path)
                renamed_local.append({"old": real_path, "new": new_path})
                logger.info(f"‚úèÔ∏è Lokal umbenannt: {real_path} ‚Üí {new_path}")
            except Exception as e:
                logger.error(f"Fehler beim lokalen Umbenennen: {real_path}: {e}")
                continue

            del UID_CACHE[source_id]

            SCAN_CACHE["invalid_names"] = [
                x for x in SCAN_CACHE["invalid_names"]
                if not (x["source"] == "local" and x.get("uid") == source_id)
            ]

            # MD5-Index aktualisieren
            for md5, entry in GLOBAL_MD5_INDEX.items():
                for item in entry["local"]:
                    if item.get("path") == real_path:
                        item["name"] = new_name
                        item["path"] = new_path
                        item["sanitized_name"] = sanitize_filename(new_name)
                        item["is_invalid_name"] = (item["sanitized_name"] != new_name)

        # ----------------------------------------------------------
        # B) GDRIVE rename
        # ----------------------------------------------------------
        else:
            match = next(
                (x for x in SCAN_CACHE["invalid_names"]
                 if x["source"] == "gdrive" and x["id"] == source_id),
                None
            )

            if not match:
                logger.error(f"Kein clean_name f√ºr GDrive-ID {source_id} gefunden!")
                continue

            new_name = match["clean_name"]
            try:
                drive.files().update(
                    fileId=source_id,
                    body={"name": new_name},
                    fields="id,name"
                ).execute()

                renamed_gdrive.append({"id": source_id, "new": new_name})
                logger.info(f"‚úèÔ∏è GDrive umbenannt: {source_id} ‚Üí {new_name}")

            except Exception as e:
                logger.error(f"GDrive-Umbenennfehler bei {source_id}: {e}")
                continue

            # SCAN_CACHE invalid_names aktualisieren
            SCAN_CACHE["invalid_names"] = [
                x for x in SCAN_CACHE["invalid_names"]
                if not (x["source"] == "gdrive" and x["id"] == source_id)
            ]

            # MD5-Index aktualisieren
            for md5, entry in GLOBAL_MD5_INDEX.items():
                for item in entry["gdrive"]:
                    if item.get("id") == source_id:
                        item["name"] = new_name
                        item["sanitized_name"] = sanitize_filename(new_name)
                        item["is_invalid_name"] = (item["sanitized_name"] != new_name)

    deleted_local = []
    deleted_gdrive = []
    synced_local = []
    synced_gdrive = []
    errors = []

    # ======================================================
    # 1) SYNC verarbeiten
    # ======================================================
    sync_ids = form.getlist("sync_ids")
    for sid in sync_ids:

        # A) GDrive ‚Üí Local
        if "/" not in sid:
            res = await sync_from_gdrive(sid)
            if res:
                synced_local.append(res)

        # B) Local ‚Üí GDrive
        else:
            res = await sync_to_gdrive(sid)
            if res:
                synced_gdrive.append(res)

    # ======================================================
    # 2) SCAN_CACHE Eintr√§ge f√ºr gesyncte Dateien entfernen
    # ======================================================
    if sync_ids:
        old_invalid = SCAN_CACHE.get("invalid_md5", [])
        new_invalid = []

        for item in old_invalid:
            md5 = item["md5"]

            local_paths = [x.get("path") for x in item["local"]]
            gdrive_ids = [x.get("id") for x in item["gdrive"]]

            # wenn irgendein Teil dieses Eintrags gesynct wurde ‚Üí entfernen
            if any(sid in local_paths or sid in gdrive_ids for sid in sync_ids):
                logger.info(f"üßπ Entferne aus SCAN_CACHE wegen Sync: {md5}")
                continue

            new_invalid.append(item)

        SCAN_CACHE["invalid_md5"] = new_invalid

    # ======================================================
    # 3.a) DELETE verarbeiten
    # ======================================================

    delete_ids = form.getlist("delete_ids")
    for did in delete_ids:

        # lokal
        if did.startswith("/") and os.path.exists(did):
            try:
                os.remove(did)
                deleted_local.append(did)
                logger.info(f"üóë Lokal gel√∂scht: {did}")
            except Exception as e:
                msg = f"Fehler lokales L√∂schen {did}: {e}"
                errors.append(msg)

        # GDrive
        else:
            try:
                drive.files().delete(fileId=did).execute()
                deleted_gdrive.append(did)
                logger.info(f"üóë GDrive gel√∂scht: {did}")
            except Exception as e:
                msg = f"Fehler GDrive-L√∂schen {did}: {e}"
                errors.append(msg)

    # ======================================================
    # 3.b) UNIQUE FILENAMES ‚Üí echte Umbenennung
    # ======================================================

    unique_ids = form.getlist("unique_filename_ids")
    renamed_unique = []  # optional f√ºr UI

    for uid in unique_ids:

        # üîç 1) passende Collision-Gruppe finden
        collision_group = next(
            (cg for cg in SCAN_CACHE.get("filename_collisions", [])
             if any(x.get("path") == uid for x in cg["local"])
             or any(x.get("id") == uid for x in cg["gdrive"])),
            None
        )

        if not collision_group:
            logger.error(f"‚ùå Keine Collision-Gruppe f√ºr {uid} gefunden!")
            continue

        filename = collision_group["name"]
        md5 = None

        # herausfinden, welche Datei es ist
        entry_local = next((x for x in collision_group["local"] if x.get("path") == uid), None)
        entry_gdrive = next((x for x in collision_group["gdrive"] if x.get("id") == uid), None)

        if entry_local:
            md5 = entry_local["md5"]
        if entry_gdrive:
            md5 = entry_gdrive["md5"]

        if not md5:
            logger.error(f"‚ùå Kein MD5 gefunden f√ºr {uid}")
            continue

        new_name = f"{md5}_{filename}"

        # ====================================================
        # A) LOKAL
        # ====================================================
        if entry_local:
            old_path = entry_local["path"]
            folder = os.path.dirname(old_path)
            new_path = os.path.join(folder, new_name)

            try:
                os.rename(old_path, new_path)
                logger.info(f"üîÅ Lokal eindeutig umbenannt: {old_path} ‚Üí {new_path}")

                renamed_unique.append({"old": old_path, "new": new_path})

                # UPDATE GLOBAL_MD5_INDEX
                for md5_key, grp in GLOBAL_MD5_INDEX.items():
                    for item in grp["local"]:
                        if item.get("path") == old_path:
                            item["name"] = new_name
                            item["path"] = new_path

            except Exception as e:
                errors.append(f"Fehler beim lokalen eindeutigen Umbenennen {old_path}: {e}")
                continue

        # ====================================================
        # B) GDRIVE
        # ====================================================
        if entry_gdrive:
            file_id = entry_gdrive["id"]
            try:
                drive.files().update(
                    fileId=file_id,
                    body={"name": new_name},
                    fields="id,name"
                ).execute()

                logger.info(f"üîÅ GDrive eindeutig umbenannt: {file_id} ‚Üí {new_name}")
                renamed_unique.append({"id": file_id, "new": new_name})

                # UPDATE GLOBAL_MD5_INDEX
                for md5_key, grp in GLOBAL_MD5_INDEX.items():
                    for item in grp["gdrive"]:
                        if item.get("id") == file_id:
                            item["name"] = new_name

            except Exception as e:
                errors.append(f"Fehler GDrive eindeutiges Umbenennen {file_id}: {e}")
                continue

    # Nachher die Collision-Gruppe l√∂schen, da kein Konflikt mehr
    SCAN_CACHE["filename_collisions"] = [
        cg for cg in SCAN_CACHE["filename_collisions"]
        if not any(uid == x.get("path") or uid == x.get("id")
                   for x in cg["local"] + cg["gdrive"])
    ]

    # ======================================================
    # 4) SCAN_CACHE nach DELETE aktualisieren
    # ======================================================

    old_invalid = SCAN_CACHE.get("invalid_md5", [])
    new_invalid = []

    for item in old_invalid:
        md5 = item["md5"]
        local = [x for x in item["local"] if x.get("path") not in deleted_local]
        gdrive = [x for x in item["gdrive"] if x.get("id") not in deleted_gdrive]

        lc = len(local)
        gc = len(gdrive)

        # UI reduzieren
        if (lc == 0 and gc == 0) or (lc == 1 and gc == 1):
            logger.info(f"üßπ Entfernt aus SCAN_CACHE (Delete): {md5}")
            continue

        new_invalid.append({
            "md5": md5,
            "local": local,
            "gdrive": gdrive,
            "status": f"{lc}x local, {gc}x gdrive",
        })

    SCAN_CACHE["invalid_md5"] = new_invalid

    # ======================================================
    # 5) invalid_names nach Delete filtern
    # ======================================================

    new_names = []
    for item in SCAN_CACHE.get("invalid_names", []):
        if item["source"] == "local" and item["path"] in deleted_local:
            continue
        if item["source"] == "gdrive" and item["id"] in deleted_gdrive:
            continue
        new_names.append(item)

    SCAN_CACHE["invalid_names"] = new_names

    # ======================================================
    # Filename-Collisions nach Operationen aktualisieren
    # ======================================================

    old_cols = SCAN_CACHE.get("filename_collisions", [])
    filename_collisions = []
    filename_collisions_use = []

    # Alle IDs, die in dieser Operation angefasst wurden:
    touched_ids = set(delete_ids) | set(unique_ids) | set(rename_ids)

    for item in old_cols:

        # Wenn irgendein Eintrag der Gruppe angefasst wurde ‚Üí ganze Gruppe entfernen
        group_ids = set()

        for x in item["local"]:
            if "path" in x:
                group_ids.add(x["path"])

        for x in item["gdrive"]:
            if "id" in x:
                group_ids.add(x["id"])

        if group_ids & touched_ids:
            filename_collisions_use.append(item)
            logger.info(f"üßπ Filename-Kollision vollst√§ndig entfernt: {item['name']}")
            continue  # ganze Gruppe skippen ‚Üí wird gel√∂scht

        # --- Falls nicht ber√ºhrt, wird sie √ºbernommen ---
        filename_collisions.append(item)

    SCAN_CACHE["filename_collisions"] = filename_collisions

    # ======================================================
    # 6) Ergebnis
    # ======================================================

    return templates.TemplateResponse(
        "diff_gdrive_local_done.j2",
        {
            "request": request,
            "version": VERSION,
            "renamed_local": renamed_local,
            "renamed_gdrive": renamed_gdrive,
            "deleted_local": deleted_local,
            "deleted_gdrive": deleted_gdrive,
            "synced_local": synced_local,
            "synced_gdrive": synced_gdrive,
            "filename_collisions": filename_collisions_use,
            "errors": errors,
        },
    )
